{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/large-language-model/blob/main/3%20Multi-Stage%20Reasoning%20with%20LangChain%20-%20GPT%20Neo%20based%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_YPyfUW58q"
      },
      "source": [
        "# Building Multi-stage Reasoning Systems with LangChain\n",
        "\n",
        "### Multi-stage reasoning systems\n",
        "In this notebook we're going to create two AI systems:\n",
        "- The first, code named `JekyllHyde` will be a prototype AI self-commenting-and-moderating tool that will create new reaction comments to a piece of text with one LLM and use another LLM to critique those comments and flag them if they are negative. To build this we will walk through the steps needed to construct prompts and chains, as well as multiple LLM Chains that take multiple inputs, both from the previous LLM and external.\n",
        "- The second system, codenamed `DaScie` (pronounced \"dae-see\") will take the form of an LLM-based agent that will be tasked with performing data science tasks on data that will be stored in a vector database using ChromaDB. We will use LangChain agents as well as the ChromaDB library, as well as the Pandas Dataframe Agent and python REPL (Read-Eval-Print Loop) tool.\n",
        "\n",
        "### Learning Objectives\n",
        "By the end of this notebook, you will be able to:\n",
        "1. Build prompt template and create new prompts with different inputs\n",
        "2. Create basic LLM chains to connect prompts and LLMs.\n",
        "3. Construct sequential chains of multiple `LLMChains` to perform multi-stage reasoning analysis.\n",
        "4. Use langchain agents to build semi-automated systems with an LLM-centric agent to perform internet searches and dataset analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed1F95YWW6nM"
      },
      "source": [
        "### Libraries:\n",
        "* [langchain[llms]](https://github.com/langchain-ai/langchain) is for LangChain's multi-stage reasoning.\n",
        "* [wikipedia](https://github.com/goldsmith/Wikipedia) is for accessing and parsing data from Wikipedia.\n",
        "* [google-search-results](https://github.com/serpapi/google-search-results-python) is for scraping and parsing search results from Google, Bing, Baidu, Yandex, Yahoo, Home Depot, eBay and more, using SerpApi.\n",
        "* [better-profanity](https://github.com/snguyenthanh/better_profanity) is for cleaning up swear words in strings.\n",
        "* [sqlalchemy](https://github.com/sqlalchemy/sqlalchemy) is a Python SQL toolkit and Object Relational Mapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV_a2RPSYXJU"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain[llms]==0.0.266\n",
        "!pip -q install wikipedia==1.4.0 google-search-results==2.4.2 better-profanity==0.7.0 sqlalchemy==2.0.19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztneW42xZOiB"
      },
      "source": [
        "## Generate API tokens\n",
        "For many of the services that we'll using in the notebook, we'll need some API keys. Follow the instructions below to generate your own.\n",
        "\n",
        "### Hugging Face Hub\n",
        "1. Go to this [Inference API page](https://huggingface.co/inference-api) and click \"Sign Up\" on the top right.\n",
        "\n",
        "<img src=\"https://files.training.databricks.com/images/llm/hf_sign_up.png\" width=700>\n",
        "\n",
        "2. Once you have signed up and confirmed your email address, click on your user icon on the top right and click the `Settings` button.\n",
        "\n",
        "3. Navigate to the `Access Token` tab and copy your token.\n",
        "\n",
        "<img src=\"https://files.training.databricks.com/images/llm/hf_token_page.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1y494QuN0OX"
      },
      "source": [
        "### SerpApi\n",
        "\n",
        "1. Go to this [page](https://serpapi.com/search-api) and click \"Register\" on the top right.\n",
        "<img src=\"https://files.training.databricks.com/images/llm/serp_register.png\" width=800>\n",
        "\n",
        "2. After registration, navigate to your dashboard and `API Key` tab. Copy your API key.\n",
        "<img src=\"https://files.training.databricks.com/images/llm/serp_api.png\" width=800>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny0VwMbIP2Yb"
      },
      "source": [
        "### Create Environment File\n",
        "To use LLM models and external services, we need to add access token for HuggingFace and API key for SerpApi to our environment path.\n",
        "\n",
        "1. Create <code>secret.json</code> and place it in our GDrive.\n",
        "2. Add entries, i.e. key-value pairs in the following format:\n",
        "```{code-block}\n",
        "{\n",
        "    \"HUGGINGFACEHUB_API_TOKEN\": \"<FILL IN>\",\n",
        "    \"SERPAPI_API_KEY\": \"<FILL IN>\",\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbMlFyXXZTS8"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# path to api keys, i.e. where secrets.json is stored in GDrive\n",
        "SECRET_FILE_PATH = \"/content/drive/MyDrive/secrets.json\"\n",
        "\n",
        "# load environment variables, i.e. tokens or api keys required for HuggingFace, SerpApi and OpenAI API usages\n",
        "with open(SECRET_FILE_PATH, \"r\") as f :\n",
        "    secrets = json.loads(f.read())\n",
        "\n",
        "    for (key, value) in secrets.items() :\n",
        "        os.environ[key] = value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYQ7YcfTZAJS"
      },
      "source": [
        "## `JekyllHyde` - A self moderating system for social media\n",
        "\n",
        "In this section we will build an AI system that consists of two LLMs. `Jekyll` will be an LLM designed to read in a social media post and create a new comment. However, `Jekyll` can be moody at times so there will always be a chance that it creates a negative-sentiment comment... we need to make sure we filter those out. Luckily, that is the role of `Hyde`, the other LLM that will watch what `Jekyll` says and flag any negative comments to be removed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l7IUvs4OUAc"
      },
      "source": [
        "### Step 1 - Letting Jekyll Speak\n",
        "#### Building the Jekyll Prompt\n",
        "\n",
        "To build `Jekyll` we will need it to be able to read in the social media post and respond as a commenter. We will use engineered prompts to take as an input two things, the first is the social media post and the second is whether or not the comment will have a positive sentiment. We'll use a random number generator to create a chance of the flag to be positive or negative in `Jekyll's` response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvoOLRZQY4hw"
      },
      "outputs": [],
      "source": [
        "# Let's start with the prompt template\n",
        "from langchain import PromptTemplate\n",
        "import numpy as np\n",
        "\n",
        "# Our template for Jekyll will instruct it on how it should respond, and what variables (using the {text} syntax) it should use.\n",
        "jekyll_template = \"\"\"\n",
        "You are a social media post commenter, you will respond to the following post with a {sentiment} response.\n",
        "Post:\" {social_post}\"\n",
        "Comment:\n",
        "\"\"\"\n",
        "\n",
        "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
        "jekyll_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"sentiment\", \"social_post\"],\n",
        "    template=jekyll_template,\n",
        ")\n",
        "\n",
        "# Okay now that's ready we need to make the randomized sentiment\n",
        "random_sentiment = \"nice\"\n",
        "if np.random.rand() < 0.1:\n",
        "    random_sentiment = \"mean\"\n",
        "\n",
        "# We'll also need our social media post:\n",
        "social_post = \"I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Coursera\"\n",
        "\n",
        "# Let's create the prompt and print it out, this will be given to the LLM.\n",
        "jekyll_prompt = jekyll_prompt_template.format(\n",
        "    sentiment=random_sentiment, social_post=social_post\n",
        ")\n",
        "\n",
        "print(f\"Jekyll prompt:{jekyll_prompt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wiNxt0APW4Q"
      },
      "source": [
        "### Step 2 - Giving Jekyll a brain!\n",
        "#### Building the Jekyll LLM\n",
        "\n",
        "Note: We provide an option for you to use either Hugging Face or OpenAI. If you continue with Hugging Face, the notebook execution will take a long time (up to 10 mins each cell). If you don't mind using OpenAI, following the next markdown cell for API key generation instructions.\n",
        "\n",
        "For OpenAI,  we will use their GPT-3 model: `text-babbage-001` as our LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvndshczP6of"
      },
      "outputs": [],
      "source": [
        "# # To interact with LLMs in LangChain we need the following modules loaded\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "## We can also use a model from HuggingFaceHub if we wish to go open-source!\n",
        "model_id = \"EleutherAI/gpt-neo-2.7B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=1,\n",
        ")\n",
        "\n",
        "jekyll_llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHpQuZN3ahWm"
      },
      "source": [
        "### Step 3 - What does Jekyll Say?\n",
        "#### Building our Prompt-LLM Chain\n",
        "\n",
        "We can simplify our input by chaining the prompt template with our LLM so that we can pass the two variables directly to the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3VtfFRqZJJ0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from better_profanity import profanity\n",
        "\n",
        "jekyll_chain = LLMChain(\n",
        "    llm=jekyll_llm,\n",
        "    prompt=jekyll_prompt_template,\n",
        "    output_key=\"jekyll_said\",\n",
        "    verbose=False\n",
        ")  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
        "\n",
        "# To run our chain we use the .run() command and input our variables as a dict\n",
        "jekyll_said = jekyll_chain.run(\n",
        "    {\"sentiment\": random_sentiment, \"social_post\": social_post}\n",
        ")\n",
        "\n",
        "# Printing what Jekyll said:\n",
        "print(f\"Jekyll said:{jekyll_said}\")\n",
        "\n",
        "# Let's clean it up:\n",
        "cleaned_jekyll_said = profanity.censor(jekyll_said)\n",
        "print(f\"Jekyll said (cleaned):{cleaned_jekyll_said}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlLUyey-QUM_"
      },
      "source": [
        "### Step 4 - Time for Jekyll to Hyde\n",
        "#### Building the second chain for our Hyde moderator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udH8Mdpwa3MF"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# -----------------------------------\n",
        "# 1 We will build the prompt template\n",
        "# Our template for Hyde will take Jekyll's comment and do some sentiment analysis.\n",
        "\n",
        "hyde_template = \"\"\"\n",
        "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\n",
        "Original comment: {jekyll_said}\n",
        "Edited comment:\n",
        "\"\"\"\n",
        "\n",
        "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
        "hyde_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"jekyll_said\"],\n",
        "    template=hyde_template,\n",
        ")\n",
        "# -----------------------------------\n",
        "# -----------------------------------\n",
        "# 2 We connect an LLM for Hyde, (we could use a slightly more advanced model 'text-davinci-003 since we have some more logic in this prompt).\n",
        "hyde_llm = jekyll_llm\n",
        "\n",
        "# -----------------------------------\n",
        "# -----------------------------------\n",
        "# 3 We build the chain for Hyde\n",
        "hyde_chain = LLMChain(\n",
        "    llm=hyde_llm, prompt=hyde_prompt_template, verbose=False\n",
        ")  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
        "# -----------------------------------\n",
        "# -----------------------------------\n",
        "# 4 Let's run the chain with what Jekyll last said\n",
        "# To run our chain we use the .run() command and input our variables as a dict\n",
        "hyde_says = hyde_chain.run({\"jekyll_said\": jekyll_said})\n",
        "# Let's see what hyde said...\n",
        "print(f\"Hyde says: {hyde_says}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBFRkAs9QlXA"
      },
      "source": [
        "### Step 5 - Creating `JekyllHyde`\n",
        "#### Building our first Sequential Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19D8MTQFa7KX"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# The SequentialChain class takes in the chains we are linking together, as well as the input variables that will be added to the chain. These input variables can be used at any point in the chain, not just the start.\n",
        "jekyllhyde_chain = SequentialChain(\n",
        "    chains=[jekyll_chain, hyde_chain],\n",
        "    input_variables=[\"sentiment\", \"social_post\"],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# We can now run the chain with our randomized sentiment, and the social post!\n",
        "jekyllhyde_chain.run({\"sentiment\": random_sentiment, \"social_post\": social_post})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGaTFk7_Q62n"
      },
      "source": [
        "## `DaScie` - Our first vector database data science AI agent!\n",
        "\n",
        "In this section we're going to build an Agent based on the [ReAct paradigm](https://react-lm.github.io/) (or thought-action-observation loop) that will take instructions in plain text and perform data science analysis on data that we've stored in a vector database. The agent type we'll use is using zero-shot learning, which takes in the prompt and leverages the underlying LLMs' zero-shot abilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ROyQhXaRDmH"
      },
      "source": [
        "### Step 1 - Hello DaScie!\n",
        "#### Creating a data science-ready agent with LangChain!\n",
        "\n",
        "The tools we will give to DaScie so it can solve our tasks will be access to the internet with Google Search, the Wikipedia API, as well as a Python Read-Evaluate-Print Loop runtime, and finally access to a terminal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7OCpJetbCES"
      },
      "outputs": [],
      "source": [
        "# For DaScie we need to load in some tools for it to use, as well as an LLM for the brain/reasoning\n",
        "from langchain.agents import load_tools  # This will allow us to load tools we need\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import (\n",
        "    AgentType,\n",
        ")  # We will be using the type: ZERO_SHOT_REACT_DESCRIPTION which is standard\n",
        "\n",
        "# if use Hugging Face\n",
        "llm = jekyll_llm\n",
        "\n",
        "tools = load_tools([\"wikipedia\", \"serpapi\", \"python_repl\", \"terminal\"], llm=llm)\n",
        "\n",
        "# We now create DaScie using the \"initialize_agent\" command.\n",
        "dascie = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCkWYupPbxdt"
      },
      "source": [
        "### Step 2 - Testing out DaScie's skills\n",
        "Let's see how well DaScie can work with data on Wikipedia and create some data science results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM79AfPlb8rd"
      },
      "outputs": [],
      "source": [
        "dascie.run(\n",
        "    \"Create a dataset (DO NOT try to download one, you MUST create one based on what you find) on the performance of the Mercedes AMG F1 team in 2020 and do some analysis. You need to plot your results.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBpK1zBqRdPF"
      },
      "outputs": [],
      "source": [
        "# Let's try to improve on these results with a more detailed prompt.\n",
        "dascie.run(\n",
        "    \"Create a detailed dataset (DO NOT try to download one, you MUST create one based on what you find) on the performance of each driver in the Mercedes AMG F1 team in 2020 and do some analysis with at least 3 plots, use a subplot for each graph so they can be shown at the same time, use seaborn to plot the graphs.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czmB-tedcInH"
      },
      "source": [
        "### Step 3 - Using some local data for DaScie.\n",
        "Now we will use some local data for DaScie to analyze.\n",
        "\n",
        "\n",
        "For this we'll change DaScie's configuration so it can focus on pandas analysis of some world data. Source: https://www.kaggle.com/datasets/arnabchaki/data-science-salaries-2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foA_7EmicFr6"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_pandas_dataframe_agent\n",
        "import pandas as pd\n",
        "\n",
        "# data file: ds_salaries.csv\n",
        "URL = \"https://drive.google.com/file/d/1AhOb2d6zVKnmlfH-zo_Sdbnb1npuSltV/view?usp=sharing\"\n",
        "FILE_PATH = \"https://drive.google.com/uc?export=download&id=\" + URL.split(\"/\")[-2]\n",
        "\n",
        "datasci_data_df = pd.read_csv(FILE_PATH)\n",
        "# world_data\n",
        "dascie = create_pandas_dataframe_agent(\n",
        "    OpenAI(temperature=0), datasci_data_df, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXFcMDCBcTxV"
      },
      "outputs": [],
      "source": [
        "# Let's see how well DaScie does on a simple request.\n",
        "dascie.run(\"Analyze this data, tell me any interesting trends. Make some pretty plots.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wxnFAvBcZoV"
      },
      "outputs": [],
      "source": [
        "# Not bad! Now for something even more complex.... can we get out LLM model do some ML!?\n",
        "dascie.run(\n",
        "    \"Train a random forest regressor to predict salary using the most important features. Show me the what variables are most influential to this model\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMaG2Ax+enzVwul0qF9P+MD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}